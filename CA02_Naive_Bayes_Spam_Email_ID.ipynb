{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CA02_Naive_Bayes_Spam_Email_ID.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCZYXwtCsL_y"
      },
      "source": [
        "This is a eMail Spam Classifers that uses Naive Bayes supervised machine learning algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "4p_DvtT7sOIr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53f0c6e5-cea7-410e-b1ad-7dfb3b7e81fe"
      },
      "source": [
        "print (\"First we must import the various packages to properly run our code.\")\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First we must import the various packages to properly run our code.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0cX-zCnFCR5",
        "outputId": "85f28e3e-b70b-4fe4-f857-882f2223caa8"
      },
      "source": [
        "print (\"For CA02, we will mount our google drive in order to access the relaetd data files.\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For CA02, we will mount our google drive in order to access the relaetd data files.\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXknSIrLvzfQ"
      },
      "source": [
        "This function builds a Dictionary of most common 3000 words from all the email content. First it adds all words and symbols in the dictionary. Then it removes all non-alpha-numeric characters and any single character alpha-numeric characters. After this is complete it shrinks the Dictionary by keeping only most common 3000 words in the dictionary. It returns the Dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "jjKF0nIMwz8_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47f748e8-4269-4a7e-cebc-d9c5e548a545"
      },
      "source": [
        "print (\"We will clean and prepare the data by removing the non-required words, expressions and symbols\")\n",
        "\n",
        "print (\"We start by generating a dictionary with the words and frequency they occur. These will be the words we will remove\")\n",
        "print (\"We create a dictionary with the 3,000 most common words\")\n",
        "def make_Dictionary(root_dir):\n",
        "  all_words = []\n",
        "  emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]\n",
        "  for mail in emails:\n",
        "    with open(mail) as m:\n",
        "      for line in m:\n",
        "        words = line.split()\n",
        "        all_words += words\n",
        "  dictionary = Counter(all_words)\n",
        "  list_to_remove = list(dictionary)\n",
        "\n",
        "  for item in list_to_remove:\n",
        "    if item.isalpha() == False:\n",
        "      del dictionary[item]\n",
        "    elif len(item) == 1:\n",
        "      del dictionary[item]\n",
        "  dictionary = dictionary.most_common(3000)\n",
        "  return dictionary\n",
        "            "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We will clean and prepare the data by removing the non-required words, expressions and symbols\n",
            "We start by generating a dictionary with the words and frequency they occur. These will be the words we will remove\n",
            "We create a dictionary with the 3,000 most common words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_NG2-TpxQ1j"
      },
      "source": [
        "This function extracts feature columns and populates their values (Feature Matrix of 3000 comumns and rows equal to the number of email files). The function also analyzes the File Names of each email file and decides if it's a Spam or not based on the naming convention. Based on this the function also creates the Labelled Data Column. This function is used to extract the training dataset as well as the testing dataset and returns the Feature Dataset and the Label column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "dmVW5xNlyOFc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9077ac4-071e-4a0c-ce54-0f860444c61d"
      },
      "source": [
        "print (\"We will extracts feature columns and populates with the values. Returns the feature matrix with label columns.\")\n",
        "\n",
        "def extract_features(mail_dir):\n",
        "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
        "  features_matrix = np.zeros((len(files),3000))\n",
        "  train_labels = np.zeros(len(files))\n",
        "  count = 1;\n",
        "  docID = 0;\n",
        "  for fil in files:\n",
        "    with open(fil) as fi:\n",
        "      for i, line in enumerate(fi):\n",
        "        if i ==2:\n",
        "          words = line.split()\n",
        "          for word in words:\n",
        "            wordID = 0\n",
        "            for i, d in enumerate(dictionary):\n",
        "              if d[0] == word:\n",
        "                wordID = i\n",
        "                features_matrix[docID,wordID] = words.count(word)\n",
        "      train_labels[docID] = 0;\n",
        "      filepathTokens = fil.split('/')\n",
        "      lastToken = filepathTokens[len(filepathTokens)-1]\n",
        "      if lastToken.startswith(\"spmsg\"):\n",
        "        train_labels[docID] = 1;\n",
        "        count = count + 1\n",
        "      docID = docID + 1\n",
        "  return features_matrix, train_labels                "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We will extracts feature columns and populates with the values. Returns the feature matrix with label columns.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbOV1Y4hxpiy"
      },
      "source": [
        "The section is the main Program that calls the above two functions and gets executed first. First it \"trains\" the model using model.fit function and Training Dataset. After that it scores the Test Data set by running the Trained Model with the Test Data set. At the end it prints the model performance in terms of accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "zoq-rE7Mx0pp"
      },
      "source": [
        "TRAIN_DIR = '/content/drive/My Drive/MSBA_Colab_2020/ML_Algorithms/CA02/train-mails'\n",
        "TEST_DIR = '/content/drive/My Drive/MSBA_Colab_2020/ML_Algorithms/CA02/test-mails'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "134lmhauyQxE",
        "outputId": "b0ee881f-aa44-4c05-be44-cdcc7bda2ac0"
      },
      "source": [
        "dictionary = make_Dictionary(TRAIN_DIR)\n",
        "\n",
        "print (\"Reading and Processing emails from TRAIN and TEST folders\")\n",
        "features_matrix, labels = extract_features(TRAIN_DIR)\n",
        "test_features_matrix, test_labels = extract_features(TEST_DIR)\n",
        "\n",
        "model = GaussianNB()\n",
        "\n",
        "print (\"Training Model using Gaussian Naive-Bayes algorithm .....\")\n",
        "model.fit(features_matrix, labels)\n",
        "print (\"Training completed\")\n",
        "print (\"Testing trained model to predict Test Data labels\")\n",
        "predicted_labels = model.predict(test_features_matrix)\n",
        "print (\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
        "print (accuracy_score(test_labels, predicted_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading and Processing emails from TRAIN and TEST folders\n",
            "Training Model using Gaussian Naive-Bayes algorithm .....\n",
            "Training completed\n",
            "Testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9653846153846154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5_mPrvN586A"
      },
      "source": [
        "======================= END OF PROGRAM ========================="
      ]
    }
  ]
}